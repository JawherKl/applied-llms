# Part 1: Theory Foundations

Welcome to the foundational module of your LLM journey! This section is dedicated to building a solid, intuitive understanding of the core concepts that make Large Language Models work. As a software engineer, you know that a strong grasp of fundamentals is what allows you to debug complex issues and build robust systems, not just use APIs.

> **üéØ Goal for this Section:** To move from "It generates text magically" to "I understand the key components that enable it to generate text and why they are designed that way."

## üìö What We'll Cover (Days 1-8)

This section is structured according to the first 8 days of the roadmap:

1.  **What are LLMs?** - The high-level intuition.
2.  **NLP Basics** - Tokenization, Embeddings, and the revolutionary Attention mechanism.
3.  **LLM Architectures** - Comparing and contrasting models like GPT, BERT, and T5.
4.  **Key Terminologies** - The essential vocabulary of the LLM world.
5.  **Paper Summaries** - Simplifying groundbreaking research, starting with "Attention is All You Need."
6.  **Visual Explanations** - Learning through diagrams and videos.
7.  **Quiz Yourself** - Testing your understanding.
8.  **Summarize in Your Words** - Solidifying knowledge by writing it down.

## üó∫Ô∏è How to Navigate This Folder

Each concept has its own dedicated readme file. It is highly recommended you go through them in order, as each topic builds on the previous one.

| File | Topic | Description |
| :--- | :--- | :--- |
| [`01-what-are-llms.md`](./01-what-are-llms.md) | **What are LLMs?** | A high-level overview of how LLMs work and why they represent a paradigm shift. |
| [`02-nlp-basics.md`](./02-nlp-basics.md) | **NLP Basics** | The fundamental pillars: tokenization, word embeddings, and the attention mechanism. |
| [`03-llm-architectures.md`](./03-llm-architectures.md) | **LLM Architectures** | A comparison of Transformer-based architectures (Encoder vs. Decoder). |
| [`04-key-terminologies.md`](./04-key-terminologies.md) | **Key Terminologies** | A glossary of essential terms like parameters, fine-tuning, and hallucination. |
| [`05-paper-summaries/`](./05-paper-summaries/) | **Paper Summaries** | Directory containing simplified breakdowns of influential academic papers. |
| [`06-visual-explanations.md`](./06-visual-explanations.md) | **Visual Explanations** | A curated list of the best videos and visual resources for these concepts. |

## üõ†Ô∏è Mindset for Success

*   **Don't Rush:** The goal is understanding, not completion. It's better to fully grasp attention than to skim all 8 days quickly.
*   **Think Like an Engineer:** Constantly ask "How might this be implemented?" and "What are the performance implications?"
*   **Embrace the Math (a little):** You don't need to derive equations, but try to understand the intuition behind the concepts (e.g., what is the goal of the Softmax function in attention?).
*   **Use the Resources:** The visual explanations and paper summaries are there to help. If a concept in the main text is unclear, jump to a video.

## ‚úÖ How to Know You're Ready to Move On

After completing this section, you should be able to:

*   Explain at a high level how an LLM generates text.
*   Describe what a token is and why tokenization is important.
*   Explain the intuition behind word vectors (embeddings).
*   Articulate the core idea of the attention mechanism in your own words.
*   Understand the difference between a model like GPT (autoregressive) and BERT (autoencoding).
*   Confidently use terms like "parameters," "fine-tuning," and "context window" in a sentence.

---

**Ready to begin?** ‚û°Ô∏è Begin with **[What are LLMs?](./01-what-are-llms.md)**